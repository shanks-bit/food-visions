{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "food vision",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS/TeSPTstLFa/J3DQLWcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanks-bit/food-visions/blob/main/food_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuPVXWTgG9nL"
      },
      "source": [
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv1LBfkNHJPf"
      },
      "source": [
        "# Get TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNTSlxYtHPFJ"
      },
      "source": [
        "# List available datasets\n",
        "datasets_list = tfds.list_builders() # get all available datasets in TFDS\n",
        "print(\"food101\" in datasets_list) # is the dataset we're after available?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfhq63mRHSRV"
      },
      "source": [
        "# Load in the data (takes about 5-6 minutes in Google Colab)\n",
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRxDbsp_HWH5"
      },
      "source": [
        "# Take one sample off the training data\n",
        "train_one_sample = train_data.take(1) # samples are in format (image_tensor, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhR0vQLRHgys"
      },
      "source": [
        "# What does one sample of our training data look like?\n",
        "train_one_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUdqlogyHj1d"
      },
      "source": [
        "# Output info about our training sample\n",
        "for image, label in train_one_sample:\n",
        "  print(f\"\"\"\n",
        "  Image shape: {image.shape}\n",
        "  Image dtype: {image.dtype}\n",
        "  Target class from Food101 (tensor form): {label}\n",
        "  Class name (str form): {class_names[label.numpy()]}\n",
        "        \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haXRLUVhHnYw"
      },
      "source": [
        "# What are the min and max values?\n",
        "tf.reduce_min(image), tf.reduce_max(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQLsmIfKHtAR"
      },
      "source": [
        "# Plot an image tensor\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)\n",
        "plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\n",
        "plt.axis(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45cmsJeSHw85"
      },
      "source": [
        "# Make a function for preprocessing images\n",
        "def preprocess_img(image, label, img_shape=224):\n",
        "  \"\"\"\n",
        "  Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
        "  [img_shape, img_shape, color_channels]\n",
        "  \"\"\"\n",
        "  image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
        "  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfJOd_OmH0fO"
      },
      "source": [
        "# Preprocess a single sample image and check the outputs\n",
        "preprocessed_img = preprocess_img(image, label)[0]\n",
        "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\n",
        "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETbyEWqbH3BS"
      },
      "source": [
        "# We can still plot our preprocessed image as long as we \n",
        "# divide by 255 (for matplotlib capatibility)\n",
        "plt.imshow(preprocessed_img/255.)\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWlaL7bIH7KM"
      },
      "source": [
        "# Map preprocessing function to training data (and paralellize)\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map prepreprocessing function to test data\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Turn test data into batches (don't need to shuffle)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiknbQrAH_Yw"
      },
      "source": [
        "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      montior=\"val_acc\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=0) # don't print out whether or not model is being saved "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUDNMpBaIE8_"
      },
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLeB_9dDIJOs"
      },
      "source": [
        "mixed_precision.global_policy() # should output \"mixed_float16\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BTYrsTRIM_P"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False # freeze base model layers\n",
        "\n",
        "# Create Functional model \n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "# x = preprocessing.Rescaling(1./255)(x)\n",
        "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x = layers.Dense(len(class_names))(x) # want one output neuron per class \n",
        "# Separate activation of output layer so we can output float32 activations\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF1K-3jkIRXO"
      },
      "source": [
        "# Check out our model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FV1v3S8IVUZ"
      },
      "source": [
        "# Check the dtype_policy attributes of layers in our model\n",
        "for layer in model.layers:\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTOddHeIYML"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5MwggFFIbJj"
      },
      "source": [
        "# Fit the model with callbacks\n",
        "history_101_food_classes_feature_extract = model.fit(train_data, \n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPxwGdFuIeUd"
      },
      "source": [
        "# Evaluate model (unsaved version) on whole test dataset\n",
        "results_feature_extract_model = model.evaluate(test_data)\n",
        "results_feature_extract_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Z_Stn1IhUE"
      },
      "source": [
        "# Clone the model we created (this resets all weights)\n",
        "cloned_model = tf.keras.models.clone_model(model)\n",
        "cloned_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uws68DADIkIT"
      },
      "source": [
        "# Where are our checkpoints stored?\n",
        "checkpoint_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH3bYNB4IozC"
      },
      "source": [
        "# Load checkpointed weights into cloned_model\n",
        "cloned_model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfzNZZb-Iq0F"
      },
      "source": [
        "# Compile cloned_model (with same parameters as original model)\n",
        "cloned_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD86X9DfIs_S"
      },
      "source": [
        "# Evalaute cloned model with loaded weights (should be same score as trained model)\n",
        "results_cloned_model_with_loaded_weights = cloned_model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-u9KrBmIvaO"
      },
      "source": [
        "# Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving\n",
        "import numpy as np\n",
        "assert np.isclose(results_feature_extract_model, results_cloned_model_with_loaded_weights).all() # check if all elements in array are close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPsENzonIxS3"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in cloned_model.layers[1].layers[:20]: # check only the first 20 layers to save space\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2prLHVzIzRH"
      },
      "source": [
        "# Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\n",
        "save_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
        "model.save(save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC7U3hDdI35H"
      },
      "source": [
        "# Load model previously saved above\n",
        "loaded_saved_model = tf.keras.models.load_model(save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhwRp6V4I6Ob"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU-QGOUYI8rx"
      },
      "source": [
        "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
        "results_loaded_saved_model = loaded_saved_model.evaluate(test_data)\n",
        "results_loaded_saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXU9ah0I_rN"
      },
      "source": [
        "# The loaded model's results should equal (or at least be very close) to the model's results prior to saving\n",
        "# Note: this will only work if you've instatiated results variables \n",
        "import numpy as np\n",
        "assert np.isclose(results_feature_extract_model, results_loaded_saved_model).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5XdzagzJD5d"
      },
      "source": [
        "# The loaded model's results should equal (or at least be very close) to the model's results prior to saving\n",
        "# Note: this will only work if you've instatiated results variables \n",
        "import numpy as np\n",
        "assert np.isclose(results_feature_extract_model, results_loaded_saved_model).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG95MI-JJIqI"
      },
      "source": [
        "# Unzip the SavedModel downloaded from Google Stroage\n",
        "!mkdir downloaded_gs_model # create new dir to store downloaded feature extraction model\n",
        "!unzip 07_efficientnetb0_feature_extract_model_mixed_precision.zip -d downloaded_gs_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUluMbbuJKx5"
      },
      "source": [
        "# Load and evaluate downloaded GS model\n",
        "loaded_gs_model = tf.keras.models.load_model(\"/content/downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjK-9ZLtJN4g"
      },
      "source": [
        "# How does the loaded model perform?\n",
        "results_loaded_gs_model = loaded_gs_model.evaluate(test_data)\n",
        "results_loaded_gs_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocDT66yKJRuF"
      },
      "source": [
        "# Are any of the layers in our model frozen?\n",
        "for layer in loaded_gs_model.layers:\n",
        "  layer.trainable = True # set all layers to trainable\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqZQ_Yn8JUoO"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in loaded_gs_model.layers[1].layers[:20]:\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YjFXvpGJXRo"
      },
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j46g7k_JJa-x"
      },
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down \n",
        "                                                 min_lr=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2mZqHMUJeXW"
      },
      "source": [
        "# Compile the model\n",
        "loaded_gs_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
        "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
        "                        metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia6swfwGJgQ_"
      },
      "source": [
        "# Start to fine-tune (all layers)\n",
        "history_101_food_classes_all_data_fine_tune = loaded_gs_model.fit(train_data,\n",
        "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
        "                                                        steps_per_epoch=len(train_data),\n",
        "                                                        validation_data=test_data,\n",
        "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
        "                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
        "                                                                   model_checkpoint, # save only the best model during training\n",
        "                                                                   early_stopping, # stop model after X epochs of no improvements\n",
        "                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t9yz72YJkBm"
      },
      "source": [
        "# Save model locally (note: if you're using Google Colab and you save your model locally, it will be deleted when your Google Colab session ends)\n",
        "loaded_gs_model.save(\"07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyAMqIbNJp12"
      },
      "source": [
        "# Evaluate mixed precision trained loaded model\n",
        "results_loaded_gs_model_fine_tuned = loaded_gs_model.evaluate(test_data) \n",
        "results_loaded_gs_model_fine_tuned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_LOOsvgJr11"
      },
      "source": [
        "# Download and evaluate fine-tuned model from Google Storage\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9YrPrmAJxUY"
      },
      "source": [
        "# Unzip fine-tuned model\n",
        "!mkdir downloaded_fine_tuned_gs_model # create separate directory for fine-tuned model downloaded from Google Storage\n",
        "!unzip /content/07_efficientnetb0_fine_tuned_101_classes_mixed_precision -d downloaded_fine_tuned_gs_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvgpwg_eJzXX"
      },
      "source": [
        "# Load in fine-tuned model from Google Storage and evaluate\n",
        "loaded_fine_tuned_gs_model = tf.keras.models.load_model(\"/content/downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRpMR7q8J2qH"
      },
      "source": [
        "# Note: Even if you're loading in the model from Google Storage, you will still need to load the test_data variable for this cell to work\n",
        "results_downloaded_fine_tuned_gs_model = loaded_fine_tuned_gs_model.evaluate(test_data)\n",
        "results_downloaded_fine_tuned_gs_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsj7RqGBJ6qE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}